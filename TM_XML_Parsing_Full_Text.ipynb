{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install https://github.com/ElsevierDev/elsapy/archive/master.zip\n",
    "#Install necessary extensions/libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xml.etree.ElementTree â€” The ElementTree XML API\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Essential libraries\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Essential libraries to make API requests with ElsSearch\n",
    "from elsapy.elsclient import ElsClient\n",
    "from elsapy.elsprofile import ElsAuthor, ElsAffil\n",
    "from elsapy.elsdoc import FullDoc, AbsDoc\n",
    "from elsapy.elssearch import ElsSearch\n",
    "import json\n",
    "import csv\n",
    "import pprint\n",
    "import requests\n",
    "import xmltodict\n",
    "import urllib3\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Science Direct Full Text API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the doi file previously extracted from ElsSearch API\n",
    "#Our data is stored in <path>\n",
    "df = pd.read_csv(r'C:\\Users\\swath\\doi_6000.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Institute Token for subscription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Append \"apikey\" and \"insttoken\" as suggest in the ElsSearch document into a config file\n",
    "config = {\n",
    "    \"apikey\": \"2dc442325fc67f2f275ec3157ef8df65\",\n",
    " \"insttoken\": \"6beb1f6c29d85f50029bf11c8de94d1b\"\n",
    "    }\n",
    "\n",
    "client = ElsClient(config['apikey'])\n",
    "client.inst_token = config['insttoken']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create string1 and string2 to join doi with institoken to make a single URL\n",
    "string1 = \"https://api.elsevier.com/content/article/doi/\" \n",
    "string2 = \"?apiKey=2dc442325fc67f2f275ec3157ef8df65&insttoken=6beb1f6c29d85f50029bf11c8de94d1b\"\n",
    "\n",
    "#Access every DOI in the previous file and append the new URL to another column\n",
    "df['Link'] = df['DOI'].apply(lambda x: string1 + str(x) + string2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example of a Link for 1st DOI in the file\n",
    "df['Link'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the combined abstracts in another CSV file\n",
    "df.to_csv(r'C:\\Users\\swath\\OneDrive\\Desktop\\Projects\\NLP_Coal\\doi_6000_withlink.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.read_csv(r'C:\\Users\\swath\\OneDrive\\Desktop\\Projects\\NLP_Coal\\doi_6000_withlink.csv')\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A checkloop to extract the number of full-text papers returned based on previous API request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a header's dictionary to pass through requests\n",
    "headers_dict = {\"X-ELS-APIKey\": \"2dc442325fc67f2f275ec3157ef8df65\", \"X-ELS-Insttoken\": \"6beb1f6c29d85f50029bf11c8de94d1b\", \"Accept\": \"application/xml\"}\n",
    "\n",
    "#Indicators for every file\n",
    "yes = 0\n",
    "no = 0\n",
    "\n",
    "#Loop to extract every file from the list of unique (5945) DOIs\n",
    "for i in range(0, 5945):\n",
    "    \n",
    "    #x takes response of the HTTP request, passes link\n",
    "    x = requests.get(df_new['Link'][i], headers=headers_dict)\n",
    "    #print(x.text) #check\n",
    "        \n",
    "    #Save response as XML file (stored in root path of the user)\n",
    "    with open(\"full_text.xml\", 'wb') as f:\n",
    "        f.write(x.content)\n",
    "    \n",
    "    #tree calls the XML file stored at root location\n",
    "    tree = ET.parse(r'C:\\Users\\swath\\full_text.xml')\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    original_text = root.find('{http://www.elsevier.com/xml/svapi/article/dtd}originalText')\n",
    "    #print(originaltext)\n",
    "    doc = original_text.find('{http://www.elsevier.com/xml/xocs/dtd}doc')\n",
    "    #print(doc)\n",
    "    serial_item = doc.find('{http://www.elsevier.com/xml/xocs/dtd}serial-item')\n",
    "    #print(serial_item)\n",
    "\n",
    "    if serial_item != None:\n",
    "        yes +=1\n",
    "        print(yes)\n",
    "    else:\n",
    "        #print(\"Full text for this paper doesn't exist\")\n",
    "        no+=1\n",
    "        \n",
    "print(yes)\n",
    "print(no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XML Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a header's dictionary to pass through requests\n",
    "headers_dict = {\"X-ELS-APIKey\": \"2dc442325fc67f2f275ec3157ef8df65\", \"X-ELS-Insttoken\": \"6beb1f6c29d85f50029bf11c8de94d1b\", \"Accept\": \"application/xml\"}\n",
    "\n",
    "#x takes response of the HTTP request, passes link\n",
    "x = requests.get(df_new['Link'][15], headers=headers_dict)\n",
    "#print(x.text) #check\n",
    "    \n",
    "#Save response as XML file (stored in root path of the user)\n",
    "with open(\"full_text.xml\", 'wb') as f:\n",
    "    f.write(x.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['Link'][15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the data\n",
    "\n",
    "#<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "\n",
    "#tree calls the XML file stored at root location\n",
    "tree = ET.parse(r'C:\\Users\\swath\\full_text.xml')\n",
    "root = tree.getroot()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The XML file Looks like this for DOI: 10.1016/j.tet.2021.132473\n",
    "\n",
    "# <full-text-retrieval-response xmlns=\"http://www.elsevier.com/xml/svapi/article/dtd\" \n",
    "#    xmlns:bk=\"http://www.elsevier.com/xml/bk/dtd\" xmlns:cals=\"http://www.elsevier.com/xml/common/cals/dtd\" \n",
    "#    xmlns:ce=\"http://www.elsevier.com/xml/common/dtd\" \n",
    "#    xmlns:ja=\"http://www.elsevier.com/xml/ja/dtd\" \n",
    "#    xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" \n",
    "#    xmlns:sa=\"http://www.elsevier.com/xml/common/struct-aff/dtd\" \n",
    "#    xmlns:sb=\"http://www.elsevier.com/xml/common/struct-bib/dtd\" \n",
    "#    xmlns:tb=\"http://www.elsevier.com/xml/common/table/dtd\" \n",
    "#    xmlns:xlink=\"http://www.w3.org/1999/xlink\" \n",
    "#    xmlns:xocs=\"http://www.elsevier.com/xml/xocs/dtd\" \n",
    "#    xmlns:dc=\"http://purl.org/dc/elements/1.1/\" \n",
    "#    xmlns:dcterms=\"http://purl.org/dc/terms/\" \n",
    "#    xmlns:prism=\"http://prismstandard.org/namespaces/basic/2.0/\" \n",
    "#    xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\">\n",
    "# <coredata>\n",
    "# ... #This part has all basic info \n",
    "# </coredata>\n",
    "# <objects>\n",
    "# ... #This part is unnecessary\n",
    "# </objects>\n",
    "# <scopus-id>85116092071</scopus-id>\n",
    "# <scopus-eid>2-s2.0-85116092071</scopus-eid>\n",
    "# <link href=\"https://api.elsevier.com/content/abstract/scopus_id/85116092071\" rel=\"abstract\"/>\n",
    "# <originalText>\n",
    "# ... #This is the main part\n",
    "      #head\n",
    "            #title\n",
    "            #author\n",
    "            #abstract\n",
    "            #keywords\n",
    "      #body \n",
    "            #sections\n",
    "            #para\n",
    "                #subsections\n",
    "                    #para\n",
    "                \n",
    "      #tail (useless)\n",
    "# </originalText>\n",
    "# </full-text-retrieval-response>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to Extract Coredata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Coredata is present in every paper\n",
    "\n",
    "#Element.findall() finds only elements with a tag which are direct children of the current element\n",
    "#Element.find() finds the first child with a particular tag\n",
    "\n",
    "#Manually entered XML with Namespaces\n",
    "#df = pd.DataFrame({'url': url, 'doi': doi, 'title': title, 'pub_name': pub_name, 'type': type_, 'abstract': description})\n",
    "\n",
    "for entry in root.findall('{http://www.elsevier.com/xml/svapi/article/dtd}coredata'):\n",
    "    \n",
    "    url = []; doi =[]; title = []; pub_name =[]; type_ = []\n",
    "    description = []\n",
    "    #CHECK CODE\n",
    "    url.append(entry.find('{http://prismstandard.org/namespaces/basic/2.0/}url').text)\n",
    "    doi.append(entry.find('{http://prismstandard.org/namespaces/basic/2.0/}doi').text)\n",
    "    title.append(entry.find('{http://purl.org/dc/elements/1.1/}title').text)\n",
    "    pub_name.append(entry.find('{http://prismstandard.org/namespaces/basic/2.0/}publicationName').text)\n",
    "    type_.append(entry.find('{http://prismstandard.org/namespaces/basic/2.0/}aggregationType').text)\n",
    "    description.append(entry.find('{http://purl.org/dc/elements/1.1/}description').text)\n",
    "    description[0] = \" \".join(description[0].split())\n",
    "    print(url, doi, title, pub_name, type_, description)\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to extract head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_text = root.find('{http://www.elsevier.com/xml/svapi/article/dtd}originalText')\n",
    "#print(originaltext)\n",
    "doc = original_text.find('{http://www.elsevier.com/xml/xocs/dtd}doc')\n",
    "#print(doc)\n",
    "serial_item = doc.find('{http://www.elsevier.com/xml/xocs/dtd}serial-item')\n",
    "#print(serial_item)\n",
    "\n",
    "keyword_list = []\n",
    "author_list = []\n",
    "if serial_item != None:\n",
    "    article = serial_item.find('{http://www.elsevier.com/xml/ja/dtd}article')\n",
    "    #print(article)\n",
    "    head = article.find('{http://www.elsevier.com/xml/ja/dtd}head')\n",
    "    #print(head)\n",
    "    author_group = head.find('{http://www.elsevier.com/xml/common/dtd}author-group')\n",
    "    #print(author_group)\n",
    "\n",
    "    keywords_ = head.find('{http://www.elsevier.com/xml/common/dtd}keywords')\n",
    "    #print(keywords_)\n",
    "\n",
    "    for author in author_group.findall('{http://www.elsevier.com/xml/common/dtd}author'):\n",
    "        name = author.find('{http://www.elsevier.com/xml/common/dtd}given-name').text\n",
    "        surname = author.find('{http://www.elsevier.com/xml/common/dtd}surname').text\n",
    "        author_list.append(name + ' ' + surname)\n",
    "        print('\\t')\n",
    "\n",
    "    for word in keywords_.itertext():\n",
    "        keyword_list.append(word)\n",
    "\n",
    "    keyword_list = \"\".join(keyword_list)\n",
    "    keyword_list = list(keyword_list.split())\n",
    "\n",
    "    print(keyword_list)\n",
    "    print(author_list)\n",
    "\n",
    "    \n",
    "else:\n",
    "    print(\"Full text for this paper doesn't exist\")\n",
    "\n",
    "d = dict(url = np.array(url), \n",
    "         doi = np.array(doi), \n",
    "         title = np.array(title), \n",
    "         pub_name = np.array(pub_name), \n",
    "         Type = np.array(type_), \n",
    "         abstract= np.array(description), \n",
    "         authors= np.array(author_list), \n",
    "         keyword =np.array(keyword_list))\n",
    "\n",
    "#Save it in a dataframe\n",
    "df = pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in d.items() ]))\n",
    "df.fillna('', inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to extract body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body = article.find('{http://www.elsevier.com/xml/ja/dtd}body')\n",
    "#print(body) #check\n",
    "sections = body.find('{http://www.elsevier.com/xml/common/dtd}sections')\n",
    "#print(sections) #check\n",
    "\n",
    "#lists for dataframe\n",
    "label_list = [] \n",
    "title_list = []\n",
    "para_list = []\n",
    "\n",
    "#A multiple iterative loop to iterate through every section, subsection, paragraphs and sentences\n",
    "#Same logic of root to next node to root to subsequent node is followed at every loop\n",
    "\n",
    "for section1 in sections.findall('{http://www.elsevier.com/xml/common/dtd}section'):\n",
    "    if section1.find('{http://www.elsevier.com/xml/common/dtd}label') != None:\n",
    "        label = section1.find('{http://www.elsevier.com/xml/common/dtd}label').text\n",
    "        label_list.append(label)\n",
    "        print(label)\n",
    "\n",
    "        section_title_list = []\n",
    "        section_title = section1.find('{http://www.elsevier.com/xml/common/dtd}section-title')\n",
    "        for sec in section_title.itertext():\n",
    "            section_title_list.append(sec)\n",
    "\n",
    "        section_title_list = \"\".join(section_title_list)\n",
    "        section_title_list = list(section_title_list.split()) \n",
    "        section_title_list = \" \".join(section_title_list)\n",
    "        title_list.append(section_title_list)\n",
    "        print(section_title_list)\n",
    "\n",
    "        total_para = []\n",
    "        \n",
    "        #Code to extract main text before entering the subsection\n",
    "        #paragraph = section1.find('{http://www.elsevier.com/xml/common/dtd}para')\n",
    "        for paragraph in section1.findall('{http://www.elsevier.com/xml/common/dtd}para'):\n",
    "            if paragraph != None:\n",
    "                #paragraph = paragraph.text\n",
    "                #print(paragraph)\n",
    "                #print('\\n')\n",
    "                paragraph1 = []\n",
    "                for p in paragraph.itertext():\n",
    "                    paragraph1.append(p)\n",
    "\n",
    "                paragraph1 = \"\".join(paragraph1)\n",
    "                paragraph1 = list(paragraph1.split()) \n",
    "                paragraph1 = \" \".join(paragraph1)\n",
    "                total_para.append(paragraph1)\n",
    "\n",
    "        total_para = \" \".join(total_para)\n",
    "        print(total_para)\n",
    "        para_list.append(total_para)\n",
    "        print('\\n')\n",
    "        \n",
    "        #Code to extract substection paragraphs\n",
    "        for section2 in section1.findall('{http://www.elsevier.com/xml/common/dtd}section'):\n",
    "            if section2.find('{http://www.elsevier.com/xml/common/dtd}label') !=None:\n",
    "                label = section2.find('{http://www.elsevier.com/xml/common/dtd}label').text\n",
    "                label_list.append(label)\n",
    "                print('sub-section ', label)\n",
    "\n",
    "                sub_section_title_list = []\n",
    "\n",
    "                sub_section_title = section2.find('{http://www.elsevier.com/xml/common/dtd}section-title')\n",
    "                for sub in sub_section_title.itertext():\n",
    "                    sub_section_title_list.append(sub)\n",
    "\n",
    "                sub_section_title_list = \"\".join(sub_section_title_list)\n",
    "                sub_section_title_list = list(sub_section_title_list.split()) \n",
    "                sub_section_title_list = \" \".join(sub_section_title_list)\n",
    "                title_list.append(sub_section_title_list)\n",
    "                #print('sub-section-title: ', sub_section_title_list)\n",
    "\n",
    "                #Code to extract multiple paragraphs under same subsection\n",
    "                para1 = []\n",
    "                for para in section2.itertext():\n",
    "                #findall('{http://www.elsevier.com/xml/common/dtd}para'):\n",
    "                    #paragraph = para.\n",
    "                    #for i in paragraph: \n",
    "                    para1.append(para)\n",
    "\n",
    "                para1 = \"\".join(para1)\n",
    "                para1 = list(para1.split()) \n",
    "                para1 = \" \".join(para1)\n",
    "                print(para1) \n",
    "                para_list.append(para1)\n",
    "                print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dictionary to store all the values\n",
    "d = dict(url = np.array(url), \n",
    "         doi = np.array(doi), \n",
    "         title = np.array(title), \n",
    "         pub_name = np.array(pub_name), \n",
    "         Type = np.array(type_), \n",
    "         abstract= np.array(description), \n",
    "         authors= np.array(author_list), \n",
    "         keyword =np.array(keyword_list),\n",
    "         title_label = np.array(label_list),\n",
    "         titles = np.array(title_list),\n",
    "         text = np.array(para_list))\n",
    "\n",
    "df = pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in d.items() ]))\n",
    "df.fillna('', inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('C://Users//swath//Downloads//paper_xml_parsed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the text to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_dict = {} #This will create a dictionary where we will store information about the searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in root.findall('{http://www.elsevier.com/xml/svapi/article/dtd}coredata'):\n",
    "    \n",
    "    url = entry.find('{http://prismstandard.org/namespaces/basic/2.0/}url').text\n",
    "    doi = entry.find('{http://prismstandard.org/namespaces/basic/2.0/}doi').text\n",
    "    title = entry.find('{http://purl.org/dc/elements/1.1/}title').text\n",
    "    pub_name = entry.find('{http://prismstandard.org/namespaces/basic/2.0/}publicationName').text\n",
    "    type_ = entry.find('{http://prismstandard.org/namespaces/basic/2.0/}aggregationType').text\n",
    "    description = entry.find('{http://purl.org/dc/elements/1.1/}description').text\n",
    "    print(url, doi, title, pub_name, type_, description)\n",
    "    print('\\n')\n",
    "    \n",
    "    #CHECK CODE\n",
    "    info_dict['URL'] = entry.find('{http://prismstandard.org/namespaces/basic/2.0/}url').text\n",
    "    info_dict['DOI'] = entry.find('{http://prismstandard.org/namespaces/basic/2.0/}doi').text\n",
    "    info_dict['Title'] = entry.find('{http://purl.org/dc/elements/1.1/}title').text\n",
    "    info_dict['Pub_Name'] = entry.find('{http://prismstandard.org/namespaces/basic/2.0/}publicationName').text\n",
    "    info_dict['Type'] = entry.find('{http://prismstandard.org/namespaces/basic/2.0/}aggregationType').text\n",
    "    info_dict['Abstract'] = entry.find('{http://purl.org/dc/elements/1.1/}description').text\n",
    "    print(url, doi, title, pub_name, type_, description)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_text = root.find('{http://www.elsevier.com/xml/svapi/article/dtd}originalText')\n",
    "#print(originaltext)\n",
    "doc = original_text.find('{http://www.elsevier.com/xml/xocs/dtd}doc')\n",
    "#print(doc)\n",
    "serial_item = doc.find('{http://www.elsevier.com/xml/xocs/dtd}serial-item')\n",
    "#print(serial_item)\n",
    "article = serial_item.find('{http://www.elsevier.com/xml/ja/dtd}article')\n",
    "#print(article)\n",
    "\n",
    "head = article.find('{http://www.elsevier.com/xml/ja/dtd}head')\n",
    "#print(head)\n",
    "author_group = head.find('{http://www.elsevier.com/xml/common/dtd}author-group')\n",
    "#print(author_group)\n",
    "\n",
    "keywords_ = head.find('{http://www.elsevier.com/xml/common/dtd}keywords')\n",
    "#print(keywords_)\n",
    "full_name_list = []\n",
    "full_name_dict = {}\n",
    "for author in author_group.findall('{http://www.elsevier.com/xml/common/dtd}author'):\n",
    "    name = author.find('{http://www.elsevier.com/xml/common/dtd}given-name').text\n",
    "    surname = author.find('{http://www.elsevier.com/xml/common/dtd}surname').text\n",
    "    full_name_dict['Name'] = name + surname\n",
    "    full_name_list.append(full_name_dict)\n",
    "    \n",
    "keyword_list = []\n",
    "keyword_dict = {}\n",
    "\n",
    "for keyword in keywords_.findall('{http://www.elsevier.com/xml/common/dtd}keyword'):\n",
    "    keyword = keyword.find('{http://www.elsevier.com/xml/common/dtd}text').text\n",
    "    keyword_dict['Keywords'] = keyword\n",
    "    keyword_list.append(keyword_dict)\n",
    "    print(keyword_list)\n",
    "\n",
    "print(full_name_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code is run in loop until all doi are exhaisted and stored in CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
