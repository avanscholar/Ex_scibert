{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1430d809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import basic libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b789f8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Access the full text of experiment\n",
    "df = pd.read_csv(r'C:\\Users\\kumar\\OneDrive - IIT Delhi\\CPCB\\PROF. HARI\\data_final\\Raw_text_full_6000\\final_text.csv')\n",
    "#Drop the unwanted column\n",
    "df =  df.drop(['Unnamed: 0'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4612bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the duplicate entries\n",
    "df_final = df.drop_duplicates(subset = ['exp_text'], keep = 'first').reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380398b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shape of data frame\n",
    "print('Number of points: {} and column present: {}'.format(df_final.shape[0], str((df_final.columns).values[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77015af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import nltk, download repositary for tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9319ccd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the pretrained sentimental model for check the sentimental behaviour of our sentences\n",
    "import flair\n",
    "flair_sentiment = flair.models.TextClassifier.load('en-sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1db5264",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imported chemical entity extraction library\n",
    "import chemdataextractor\n",
    "from chemdataextractor import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eac5815",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_text(df):  # df is data frame of exp_text\n",
    "    #merging the entire sentences in a paragraph\n",
    "    Raw_text = df.exp_text\n",
    "    #Access the first data points\n",
    "    Raw_text_net = Raw_text[0]  \n",
    "    i = 0\n",
    "    for i in np.arange(1, df_final1.shape[0]):\n",
    "        Raw_text_net = Raw_text_net + str(Raw_text[i])\n",
    "        i = i+1\n",
    "    return Raw_text_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e6616e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Raw_text_net = merge_text(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e3a16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some details of tokenized parameter\n",
    "print('Total sentences : {} and words present: {}'.format(sent_tokenize(Raw_text_net), word_tokenize(Raw_text_net)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b972625",
   "metadata": {},
   "source": [
    "**labeling for classification: Catalyst sentences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea49dbe6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Access the actual catalyst entities\n",
    "df_catalyst = pd.read_csv(r'C:\\Users\\kumar\\OneDrive - IIT Delhi\\CPCB\\PROF. HARI\\data_final\\catalyst\\catalys_entit_final.csv')\n",
    "#Delete the unwanted columns\n",
    "df_catalyst = df_catalyst.drop(['Unnamed: 0', 'Unnamed: 3', '7738'], axis =1)\n",
    "#Catalyst entities in list form\n",
    "name_catalyst = list(df_catalyst['section name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c47886",
   "metadata": {},
   "outputs": [],
   "source": [
    "#string to be searched\n",
    "string = 'catalyst'                          \n",
    "add1 = [] \n",
    "#Sentence tokenization is done\n",
    "token_text = sent_tokenize(Raw_text_net)      \n",
    "for s in token_text:\n",
    "    if string in s:\n",
    "        #extract chemical species using chemdataextractor\n",
    "        doc = Document(s)\n",
    "        ent = doc.cems  \n",
    "        comp = []\n",
    "    for i in range(len(ent)):\n",
    "        comp.append(ent[i].text)\n",
    "    if ent != []:\n",
    "        #sentiment analysis\n",
    "        si = flair.data.Sentence(s)         \n",
    "        flair_sentiment.predict(si)\n",
    "        total_sentiment = si.labels\n",
    "        d = total_sentiment[0].to_dict()\n",
    "        #if positive labelled then moved further\n",
    "        if d['value'] == 'POSITIVE': \n",
    "            for chemical in comp:\n",
    "            if chemical in name_catalyst:\n",
    "                add1.append([s,chemical])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e0cefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make the lists comfortable in data frame\n",
    "df_cata = pd.DataFrame(add1)\n",
    "#Name the columns\n",
    "df_cata.columns = ['sentence', 'catalyst']\n",
    "#Shape of catalyst data\n",
    "print('Shape of data frame:', df_cata.shape)\n",
    "#Shows the first 5 rows\n",
    "df_cata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6ea79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the data into csv file\n",
    "df_cata.to_csv(r'C:\\Users\\Admin\\OneDrive - IIT Delhi\\CPCB\\PROF. HARI\\df_cat.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce17571f",
   "metadata": {},
   "source": [
    "**labeling for classification: Process-catalyst sentences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958300f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word to be searched\n",
    "string = 'catalyst'\n",
    "add_text = []  \n",
    "\n",
    "#Dictionary of process parameters\n",
    "process = ['temperature', 'pressure', 'weight', 'concentration', 'molar ratio',\n",
    "           'molarity','Molality','gram','atm','celcius', 'kelvin' , 'volume', 'porocity']\n",
    "replacements = {'K': 'kelvin', 'atm': 'pressure',\n",
    "               '°C': 'celcius','bar': 'pressure',\n",
    "                'g' : 'gram','m': 'molarity',\n",
    "                'M': 'Molality','Mpa': 'pressure',\n",
    "                'pa' : 'pressure', }\n",
    "\n",
    "#Deal with units of process parameter\n",
    "replacer = replacements.get\n",
    "\n",
    "#Sentence tokenization is done\n",
    "token_text = sent_tokenize(Raw_text_net)\n",
    "for s in token_text:\n",
    "    #Each sentence is word tokenized\n",
    "    sentence_list = word_tokenize(s)\n",
    "    sentence_list = [replacer(n, n) for n in sentence_list]\n",
    "    s = \" \".join(sentence_list)\n",
    "    for pro in range(len(process)):\n",
    "        if string in s:\n",
    "            if process[pro] in s:\n",
    "                #extract chemical species using chemdataextractor\n",
    "                doc = Document(s)\n",
    "                ent = doc.cems                       \n",
    "                comp = []\n",
    "                for i in range(len(ent)):\n",
    "                    comp.append(ent[i].text)           \n",
    "                if ent != []:\n",
    "                    #Sentiment analysis\n",
    "                    si = flair.data.Sentence(s)         \n",
    "                    flair_sentiment.predict(si)\n",
    "                    total_sentiment = si.labels\n",
    "                    d = total_sentiment[0].to_dict()\n",
    "                    if d['value'] == 'POSITIVE':\n",
    "                        #if positive label then add in csv\n",
    "                        for chemical in comp:\n",
    "                            if chemical in name_catalyst:\n",
    "                                add_text.append([s,process[pro], chemical])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c9949c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make the data frame\n",
    "df_pro_cata = pd.DataFrame(add_text)\n",
    "#Name the columns of data frame\n",
    "df_pro_cata.columns = ['sentence', 'process', 'catalyst']\n",
    "#Shape of this data frame\n",
    "print('Shape of data frame', df_pro_cata.shape)\n",
    "#Shows the first 5 rows\n",
    "df_pro_cata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b12b296",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the data frame into csv formate\n",
    "df_pro_cata.to_csv(r'C:\\Users\\Admin\\OneDrive - IIT Delhi\\CPCB\\PROF. HARI\\df_cat_pro.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cf678e",
   "metadata": {},
   "source": [
    "**labeling for classification: Process sentences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae9a4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of empty lists\n",
    "all_string = []\n",
    "name_process = []\n",
    "sentence_list = []\n",
    "#sentence tokenization of para\n",
    "sentences = nltk.sent_tokenize(Raw_text_net) \n",
    "\n",
    "#Dictionary of process parameters\n",
    "process = ['temperature', 'pressure', 'weight', 'concentration', 'molar ratio',\n",
    "           'molarity','Molality','gram','atm','celcius', 'kelvin' , 'volume', 'porocity']\n",
    "replacements = {'K': 'kelvin', 'atm': 'pressure',\n",
    "               '°C': 'celcius','bar': 'pressure',\n",
    "                'g' : 'gram','m': 'molarity',\n",
    "                'M': 'Molality','Mpa': 'pressure',\n",
    "                'pa' : 'pressure', }\n",
    "#Deal with units of process parameter\n",
    "replacer = replacements.get\n",
    "\n",
    "for i in range(0, len(sentences)):\n",
    "    #word tokenize to each sentence\n",
    "    sentence_list = nltk.word_tokenize(sentences[i])\n",
    "    sentence_list = [replacer(n, n) for n in sentence_list]\n",
    "    for elem in process:\n",
    "        if elem in sentence_list:\n",
    "            #sentiment analysis\n",
    "            si = flair.data.Sentence(sentences[i])        \n",
    "            flair_sentiment.predict(si)\n",
    "            total_sentiment = si.labels\n",
    "            d = total_sentiment[0].to_dict()\n",
    "            if d['value'] == 'POSITIVE':\n",
    "                #Positive sentences\n",
    "                all_string.append(sentences[i])\n",
    "                name_process.append(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826ded49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make the data frame\n",
    "df_pro = pd.DataFrame()\n",
    "#Name the columns of data frame\n",
    "df_pro['sentence']  =  all_string\n",
    "df_pro['process_name'] = name_process\n",
    "#Shape of this data frame\n",
    "print('Shape of data frame', df_pro.shape)\n",
    "df_pro.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a15ce71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the data frame into csv formate\n",
    "df.to_csv(r'C:\\Users\\Admin\\OneDrive - IIT Delhi\\CPCB\\PROF. HARI\\df_pro.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31164036",
   "metadata": {},
   "source": [
    "**Labelling for classification: neutral sentences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384557d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#string to be searched\n",
    "string = 'catalyst'                          \n",
    "add0 = [] \n",
    "#Sentence tokenization is done\n",
    "token_text = sent_tokenize(Raw_text_net)      \n",
    "for s in token_text:\n",
    "    if string in s:\n",
    "        #sentiment analysis\n",
    "        si = flair.data.Sentence(s)         \n",
    "        flair_sentiment.predict(si)\n",
    "        total_sentiment = si.labels\n",
    "        d = total_sentiment[0].to_dict()\n",
    "        #if positive labelled then moved further\n",
    "        if d['value'] == 'POSITIVE': \n",
    "            add0.append([s])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3748af07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make the lists comfortable in data frame\n",
    "df_neutral = pd.DataFrame(add0)\n",
    "#Name the columns\n",
    "df_neutral.columns = ['sentence']\n",
    "#Shape of catalyst data\n",
    "print('Shape of data frame:', df_neutral.shape)\n",
    "#Shows the first 5 rows\n",
    "df_neutral.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9496367",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the data into csv file\n",
    "df_cata.to_csv(r'C:\\Users\\Admin\\OneDrive - IIT Delhi\\CPCB\\PROF. HARI\\df_neutral.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf97a08",
   "metadata": {},
   "source": [
    "**concatenate this three files, catalyst, process, catalyst-process**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e586013",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pro = pd.read_csv(r'C:\\Users\\kumar\\OneDrive - IIT Delhi\\CPCB\\PROF. HARI\\data_final\\CLASSIFICATION\\df_pro.csv')\n",
    "#df_pro.columns = ['Unnamed: 0','sentence', 'process']\n",
    "df_pro_cata = pd.read_csv(r'C:\\Users\\kumar\\OneDrive - IIT Delhi\\CPCB\\PROF. HARI\\data_final\\CLASSIFICATION\\df_cat_pro.csv')\n",
    "df_cata = pd.read_csv(r'C:\\Users\\kumar\\OneDrive - IIT Delhi\\CPCB\\PROF. HARI\\data_final\\CLASSIFICATION\\df_cat.csv')\n",
    "df_neutral = pd.read_csv(r'C:\\Users\\kumar\\OneDrive - IIT Delhi\\CPCB\\PROF. HARI\\data_final\\CLASSIFICATION\\df_netrual.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec119c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of data points in each file\n",
    "print('Number of data points in each file catalyst: {}, process: {}, process-catalyst: {}, neutral: {}'.format(df_pro.shape[0], df_pro_cata.shape[0], df_cata.shape[0], df_neutral.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfc1df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete the dupicates from each file separately files\n",
    "df_pro = df_pro.drop_duplicates(['sentence'], keep ='first')\n",
    "df_cata = df_cata.drop_duplicates(['sentence'], keep ='first')\n",
    "df_pro_cata = df_pro_cata.drop_duplicates(['sentence'], keep ='first')\n",
    "df_neutral = df_neutral.drop_duplicates(['sentence'], keep ='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d33576",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of data points in each file after delete the duplicates\n",
    "print('Number of data points in each file catalyst: {}, process: {}, process-catalyst: {}, neutral: {}'.format(df_pro.shape[0], df_pro_cata.shape[0], df_cata.shape[0], df_neutral.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaafbdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Name of columns in each files\n",
    "print('Name of columns of each file \\n catalyst: {}\\n process: {}\\n process-catalyst: {}\\n neutral: {}\\n'.format(df_cata.columns.values,df_pro.columns.values, df_pro_cata.columns.values, df_neutral.columns.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20168a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying to make same columns\n",
    "df_cata['process'] = 'nan'\n",
    "df_pro['catalyst'] = 'nan'\n",
    "\n",
    "df_neutral['process'] = 'nan'\n",
    "df_neutral['catalyst'] = 'nan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1ab494",
   "metadata": {},
   "outputs": [],
   "source": [
    "#labeling as per requirements\n",
    "df_pro['label'] = 0\n",
    "df_cata['label'] = 1\n",
    "df_pro_cata['label'] = 2\n",
    "df_neutral['label'] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0f26b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Name of columns in each files after rearrangement of columns\n",
    "print('Name of columns of each file \\n catalyst: {}\\n process: {}\\n process-catalyst: {}\\n neutral: {}\\n'.format(df_cata.columns.values,df_pro.columns.values, df_pro_cata.columns.values, df_neutral.columns.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a46f305",
   "metadata": {},
   "outputs": [],
   "source": [
    "#column matters in concatenation of data frames\n",
    "df_cata = df_cata[['Unnamed: 0','sentence','process', 'catalyst', 'label']]\n",
    "df_pro.columns =['Unnamed: 0', 'sentence', 'process', 'catalyst', 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a05520",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To balance the data\n",
    "df_neutral = df_neutral.sample(3000,  random_state=2).reset_index(drop =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a932dadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenating all data frames along rows\n",
    "df_annotating_data = pd.concat([df_pro,df_pro_cata, df_cata, df_neutral], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92da508",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annotating_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564cc054",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete the umwanted columns\n",
    "df_annotating_data.drop(['Unnamed: 0', 'process', 'catalyst'], axis='columns', inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9501d046",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check of unique values\n",
    "df_annotating_data.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7809fe89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the data frame into csv\n",
    "df_annotating_data.to_csv(r'C:\\Users\\Admin\\OneDrive - IIT Delhi\\CPCB\\PROF. HARI\\df_final_classification.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
