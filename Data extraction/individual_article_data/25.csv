,sections,Text
0,Introduction,"Knowledge of the solubility of gases in chemical substances is important from different perspectives. The design and operation of industrial plants require experimental and theoretical studies on gas solubility. Having trustworthy information about phase equilibrium and gas solubility is also needed to perform chemical, environmental, and biological studies [1–5]. Measuring the solubility of gas is expensive, and in some cases (high temperature or/and high pressure) difficult; thus, accurate models are needed to reduce research costs. Hydrogen (H2) as a carbon-free source of energy is utilized in many industrial processes such as hydrogenation, hydro-cracking, hydro-visbreaking, and hydrotreatment. Information regarding the solubility of H2 in different solvents like alcohols, hydrocarbon fuels, etc., is an important factor in designing and operating processing units in industrial plants. The solubility of H2 can greatly influence the applications of solvents [6–8]. An organic compound containing one or more hydroxyl (−OH) groups attached to a carbon atom is called alcohol. In the chemicals industry, alcohols are very important substances in producing pharmaceuticals, perfume, cosmetics, flavor, and many others. As mentioned, the solubility of H2 in alcohols can affect the quality and applications of these products; thus, accurate predictions regarding this issue are needed. In a slurry reactor, catalytic hydrogenation reactions to convert aldehydes to alcohols usually occur. In this case, three phases, including solvents, reactants, and reaction products (liquid), hydrogen (gas), and catalyst (solid), are present. Catalyst activity and selectivity, and velocity of the reaction are influenced by alcohols that are utilized as solvents for the products of the reaction. This impact is associated with the H2 solubility in the solvents [9]. Nevertheless, H2 solubility measurements in solvents and accurate estimations with the traditional mode of equations of state (EOSs) at elevated temperatures and pressures face serious challenges and difficulties. Disadvantages such as limited flexibility, iterative calculations, and adjustable parameters make the use of common traditional methods such as EOSs unreliable and convince researchers to look for better modeling methods for gas solubility [10–15]. The solubility of gases such as carbon dioxide, nitrogen, methane, propane, butane, etc., in alcohols has been studied over the past years [16–19]. Also, the solubility of H2 in alcohols has been experimentally investigated by researchers [1,2,5–7,9,20–23]. The methods for estimation of H2 solubility in alcohols are often based on the utilization of semi-empirical and empirical models like EOSs. Liu et al. [24] measured the solubility of H2 in liquid methanol. They calculated Henry's law constants and proposed simple empirical correlations for solubility data. They demonstrated that Henry's law constants decrease with increasing temperature [24]. d’Angelo and Francesconi [9] measured the solubility of H2 in several n-alcohols (i.e., 1-butanol, 1-propanol, ethanol, and methanol). They also presented a modified method based on Peng-Robinson (PR) EOS and Henry’s law for modeling H2 solubility in n-alcohols. They indicated that temperature and pressure have increasing effects on the solubility of H2 in alcohols [9]. Jaatinen et al. [25] examined the H2 solubility in 2-propanol and modeled the data with Perturbed-Chain Statistically Associating Fluid Theory (PC-SAFT), Soave-Redlich-Kwong (SRK), and PR EOSs. They showed that the estimation of H2 solubility data using the PC-SAFT model had the highest accuracy after binary interaction parameter regression. Without it, the predictions would not be accurate [25]. Safamirzaei et al. [26] modeled H2 solubility in several alcohols named ethanol, methanol, 1-butanol, and 1-propanol applying simple correlations, EOSs, and artificial neural networks (ANNs). The results of this study showed that ANNs have better performance and accuracy than other methods in modeling H2 in alcohols [26]. Trinh et al. [27] collected a big data bank for H2 solubility in hydrocarbon and oxygenated solvents to evaluate the vapour-liquid equilibrium behavior of H2 in various solvents after converting all the data into Henry’s constants. They found that Henry's constants decrease with molecular weight and depend on the chemical family as follows: diols > alcohols > esters > aldehydes > ethers > alkanes [27]. Wu et al. [28] investigated the solubilities of H2 and carbon monoxide (CO) in methanol. Their results showed that the H2 solubility in methanol is lower than carbon monoxide solubility in methanol. They also used a modified PR EOS to estimate the gas-liquid equilibrium and estimated experimental data with a 5% error [28]. Hinkle et al. [29] used molecular simulation combined with data analysis to predict the solubilities of H2 and CO in n-alkanes and n-alcohols. Their results indicated that molecular dynamics simulations could be a fairly accurate method to estimate such systems, especially for studying a solute in a family of solvents [29]. According to the literature review, the solubility of H2 in alcohols has been studied more experimentally, and its prediction methods have been very limited, and the presented models were based on limited systems and a small number of solvents. Due to the disadvantages of classical EOSs and complex calculations of proposed precise techniques such as SAFT models, modified Henry’s law, etc., gathering more H2 solubility data for various alcohols and applying artificial intelligence, which has been shown to be very practical in many engineering sciences [30–37], can lead to a comprehensive model for the solubility of H2 in alcohols. Therefore, accurate modeling of the solubility of H2 in alcohols using intelligent models and based on more experimental H2 solubility data collected from different H2-alcohol systems and more influential parameters are very important to the design of the hydrogenation process, which is a key objective of this study. In the current study, first, a complete set of H2 solubility data (673 experimental data points) for 26 different alcohols or alcohol-containing solvents was gathered from the literature. Then, for modeling H2 solubility in alcohols, four advanced machine learning models, namely deep echo state network (DeepESN), extreme gradient boosting (XGBoost), extreme learning machine (ELM), and multivariate adaptive regression splines (MARS) are implemented in this work. Four well-known equations of state (EOSs), namely Redlich-Kwong (RK), 3-parameter Soave-Redlich-Kwong (SRK3), 3-parameter Peng-Robinson (PR3), and Schmidt-Wenzel (SW), are also utilized to have a comparison between the EOSs and machine learning models. Furthermore, the validity of the suggested models is investigated by several graphical error analyses and statistical criteria. Also, the relative influence of input parameters on H2 solubility in alcohols is checked by the relevancy factor as a sensitivity analysis. Eventually, the applicability scope of the best model of this study is investigated by the Leverage approach. Fig. 1 shows the general sketch of the problem under study and the steps of conducting this research."
1,Data acquisition,"To develop a precise comprehensive model for the estimation of H2 solubility in alcohols, 673 experimental H2 solubility data for 26 different alcohols or alcohol-containing solvents were gathered from the literature [7,9,20–23,38–52]. Table 1 represents the properties of the alcohols utilized for modeling H2 solubility in this survey. The molecular structures of these alcohols, along with their 3D illustration, are also presented in Fig. 2 . For modeling, it is assumed that H2 solubility (in terms of mole fraction) is a function of five independent variables: pressure (MPa), temperature (K), molecular weight (g/mol), critical temperature (K), and critical pressure (MPa) of alcohols. The availability of these parameters is suitable for creating a good database of H2 solubility data for model training, and after developing the models, having these parameters, we can predict the H2 solubility in the desired alcohol. Table 2 reports the statistical details of target/input parameters of the dataset utilized for modeling. Achieving a comprehensive model for estimating the solubility of H2 in alcohols requires a large set of solubility data from different systems. The properties of the 26 different alcohols or alcohol-containing solvents applied for modeling in this paper are listed in Table 1. These solvents include aliphatic alcohols, fatty alcohols, glycols, hydroxypolyether, and diols. The molecular weight of these alcohols is from 32 to 242 g/mol. The experimental dataset is also prepared in wide ranges of operating temperatures, 213–524 (K), and pressures, 0.1–110.3 (MPa). Examining the statistical information represented in Table 2, it can be seen that the variety and distribution of model input variables are broad adequate to develop a comprehensive model for predicting H2 solubility in alcohols."
2,Model development,
3,Multivariate adaptive regression splines (MARS),"3.1 Multivariate adaptive regression splines (MARS) The MARS model detailed here is based on three studies in the literature [53–55]. One-dimensional basis functions (BFs) for the piecewise linear are utilized in MARS to delineate the existing dependencies between a set of predictors and a response variable. Every estimator parameter is divided into subgroups considering the knots (every knot specifies an inflection point as well as the predictors' area). To elude breaks or sudden steps in the fitted function, there will be a changing gradient of the linear sections between every sequential couple of knots. In MARS, the choice of BFs hinges on the database and is distinct to the given problem, thereby ensuring a robust adaptive regression approach that is appropriate to solve high-dimensional, intricate problems. In this nonparametric approach, no particular dependencies between the variables are presumed. When constructing the model, BFs are adjusted in such a manner that subtractive and additive impacts of the estimators are acknowledged to calculate the answer. The 1D BFs for the MARS's shortened piecewise linear is described by [56]: (1) x - τ + = x - τ , i f x > τ , 0 , o t h e r w i s e , x - τ - = τ - x , i f x < τ , 0 , o t h e r w i s e , here, (x, τ ∈ R), and τ shows a univariate knot. The two functions named “reflected pair”, “−” and “+” are, respectively, an indication of taking just the negative and positive components and zero otherwise [53,57]. The common form linking the predictor variables and the corresponding response is shown as follows: (2) Y = f X + ε where ε denotes an additional random variable with finite variance and zero average, X = (x1, x2,…, xp)T represents the vector of estimators, and Y shows the response variable. The objective in MARS is to generate mirrored couples for any input Xj(j = 1, 2,…, p) using p-dimensional knots τi = (τi,1, τi,2, …, τi,p)T equal or close to input Xi = (xi,1, xi,2,…, xi,p)T of that input (i = 1, 2,…, N) [53,57]. The MARS's set of 1D BFs can be described as: (3) C : = ( x j - τ ) + , ( x j - τ ) - , τ ∈ x 1 , j , x 2 , j , . . . , x N , j , j ∈ 1 , 2 , . . . , p here, p represents the dimension of input space, and N shows the total number of observations and note that f(X) in Eq. (2) stands for a linear structure sequentially created by the collection C and the interception of β0 and is described by [56]: (4) Y = β 0 + ∑ m = 1 M β m B m ( X m ) + ε In the above equation, a BF or a multiplication of two BFs selected from collection C is denoted by Bm so that Bm is chosen from the collection M, which is linearly independent of BF. Also, Xm represents a vector from X which contributes to the component Bm , and βm expresses an unspecified parameter for the mth BF, except if m = 0, this will be equal to 1. Note that the product of an established BF by another mirrored couple embracing the other parameter leads to generating a new BF, implying that distinct estimator parameters interact explicitly. In addition, the model incorporates both newly produced and existing BFs. Consequently, higher-dimensional curve matching will be acquired, hence resulting in multivariable spline BFs in the form of [56]: (5) B m X m : = ∏ j = 1 K m S kj m . x kj m - τ kj m + here, Km represents the number of shortened linear functions multiplied in the mth BF, τκ m expresses the knot location for xκ m , which xκ m denotes the input parameter relating to the kth shortened linear function in the mth BF, and, finally, s κ m ∈ { ± 1 } . The “forward and backward pass” processes are two stages of MARS when constructing a regression model. In the former, the system selects the knot and its related couple of BFs that leads to the most reduction in residual error. In addition, the products fulfilling the aforementioned criteria are consecutively applied to the model to achieve a predetermined parameter Mmax [53,57]. Whereas the downside of the forward pass is overfitting, the backward pass can help preclude it by reducing the model intricacy without sacrificing the fitting accuracy. To this end, the BFs leading to the smallest augment in residual summation of squares are eliminated at every single stage through the backward process. As a result, predictors can be thoroughly removed from the system if their BFs do not majorly assist in the model's predictive accuracy. This iteration is carried on until the final model achieves the optimum number of effective terms [53,57]. A lack-of-fit (LOF) condition determined by generalized cross-validation (GCV) is used to pick the best-estimated model, f, with the optimal number of terms α (which leads to the superior forecasting match) [56]: (6) L O F f ̂ α = G C V α : = ∑ i = 1 N y i - f ̂ α X i 2 1 - Q α / N 2 where Q(α) = u + dK where K denotes the count of knots chosen in the forward pass, u expresses the count of linearly independent equations in the system, N is the number of sample data, and d refers to the cost for any BF improvement."
4,Extreme gradient boosting (XGBoost),"3.2 Extreme gradient boosting (XGBoost) XGBoost is a gradient boosting decision tree-based approach that employs a combination of classification and regression trees (CARTs) to match training datasets by lowering a regularized objective function. Each cart in CARTs is made up of three major components including root node, leaf nodes, and internal nodes, as shown in Fig. 3 . The root node, which contains the original dataset, is subdivided into internal nodes using the binary decision technique, whilst the leaf nodes display the last classifications. In gradient boosting, a sequence of basic CATRs is produced sequentially, with the weight of every particular CART requiring tweaking via the training phase [58]. The output y for a dataset is modeled by training a set of n tress according to [59]: (7) y ^ i = ∑ k = 1 N f k X i , f k ∈ f W i t h f = f ( X ) = ω q ( x ) , ( q : R m → T , ω ∈ R T ) here m, n, ω, f, fk , and T refer to the dimensions of features and samples, the weight of the leaf, space of regression trees, kth independent tree, and the count of leaves of the tree, respectively. The decision law q(x) maps the sample x to the dyadic leaf index. To determine the combination of trees, the regularized objective function is minimized through [59]: (8) L = ∑ i n l ( y ^ i , y i ) + ∑ k N Ω ( f k ) W i t h Ω ( f ) = γ T + 1 2 λ ω 2 where γ denotes the least loss decrement needed to divide a new leaf, l shows a variational convex dissipation function; Ω represents the normalization parameter which restricts the model's sophistication and decreases the possibility of overfitting; and λ refers to the regulation coefficient. In these collections of formulas, λ and γ aid to reduce the variation of model and sores overfitting [59]. The objective function for every leaf is reduced via gradient boosting, thus new branches will be created repeatedly. (9) L ( t ) = ∑ i = 1 n l ( y i , y ^ i ( t - 1 ) ) + f t ( X i ) + Ω ( f t ) here t shows the training stage's t-th repetition. The XGBoost technique, which is commonly referred to as ""greedy algorithm,"" greedily adds the space of regression trees to substantially enhance the ensemble method. As a result, by minimizing the objective function, the obtained data is repeatedly modified: (10) y ^ i ( t ) = y ^ i ( t - 1 ) + f t ( X i ) After every stage of boosting the XGBoost gets the advantage of the shrinkage strategy in which newly added weights are scaled by a learning parameter rate. This minimizes the probability of overfitting by imprinting the fingerprint of future new trees on each current tree [60]."
5,Extreme learning machine (ELM),"3.3 Extreme learning machine (ELM) The ELM [61] as a learning method is described by a single forward hidden layer natural network. This method was first relied on the mono hiding layer feed-forward smart system to establish the weight of hiding neurons at random leading to an obtained weight using a pseudo-inverse matrix as demonstrated in Fig. 4 . In Eq. (11), the count of neurons in the concealed layer is indicated by L, yl represents the borrower’s repayment condition, f (x) shows a function of the ELM, wi indicates the weights of the input and hidden layers, xj stands for the borrower’s index features (j = 1, 2, ⋯, N), βi shows the output layer weights and output of the hidden layer, bi represents the bias of the input and concealed layers, and g(wixj + bi ) is the activation function [61,62]. (11) f x j = ∑ i = 1 L β i g w i x j + b i = y l According to Eq. (11), βi should be obtained to specify the credit quality score of the borrower. The input weight wi and bias bi in the learning machine are randomly generated while the accuracy of default judgment is not affected. g(wi x + bi) is the Gaussian radial BF which is shown as below [61]: g ( w i x + b i ) = exp - b i x - w i 2 To simplify the derivation, the linear equations stated above can be defined as: (13) y l = H β Moreover, (14) H w 1 , w i , . . . w L , x 1 , x 2 , . . . x N , b 1 , b 2 , . . . b L = g w 1 x 1 + b 1 . . . g w L x N + b L . . . . . . . . . g ( w 1 x N + b 1 , . . . g ( w L x N + b L , (15) β = β 1 , β 2 , . . . β N T To learn the single-layer neural network, the hidden layer bias and input weight are iterated based on the gradient algorithm in which the error is minimized: (16) H ( w 1 , w 2 , . . . , w N , b 1 , b 2 , . . . , b N ) β - y = min w , b , β H ( w 1 , w 2 , . . . , w N , b 1 , b 2 , . . . , b N ) β - y The minimum norm least square method was proposed [61] to find a solution to this problem which no longer requires a perpetual adjustment to the input weight wi and bias bi . The solution is finally transformed to a generalized inverse problem for solving the matrix. By invoking an extreme learning machine which can relate the weight β of the hidden and output layers, it is inferred that the count of training samples in the majority of cases is significantly greater than the count of neurons in the hiding layer. Consequently, H cannot be considered a complete rank matrix and, thus, β is not a unique solution. Under these circumstances, the extended Moore–Penrose inverse H+ is undertaken to solve β as [61]: (17) β = H + y here, H+ represents the pseudo-inverse matrix or generalized inverse of matrix H. If the inverse HTH exists, H+ can be described as [61]: (18) H + = H T H T H - 1 This can be subsequently achieved with β [61]: (19) β = H T H T H - 1 y"
6,Deep echo state network (DeepESN),"3.4 Deep echo state network (DeepESN) The overall architecture of a recurrent neural network such as the echo state network (ESN) is fundamentally comprised of three components: (I) input layer, (II) a large re-current layer known as a reservoir with constant sparse hidden-to-hidden links, and (III) output layer [63] as depicted in Fig. 5 . Considering D, as the count of the input's layer neurons as well as L and N as the number of the output's layer, and the reservoir's neurons, respectively. The weights of reservoir-to-reservoir and input-to-reservoir are gathered by an N-by-N matrix Wres and an N-by-D matrix Win . Here, the weights from reservoir-to-output and input-to-output are merged in a single L-by-(D + N) matrix Wout . Only Win is chosen at random from an identical distribution [-1,1], and Wres is described by Eq. (23). Win and Wres are constant throughout the training step, and only Wout is required to be modified [64]. Training an ESN is achieved by a 2-stage controlled learning process: (I) the projection of sequences of D-dimensional input data u into the reservoir, followed by its derivation to acquire sequences of echo states x, and (II) learning matrix Wout by conventional regression procedures. In this study, an ESN with leaky-integrator neurons [65] is utilized, which was also exploited by other researchers [64,66,67]. (20) z t + 1 = W res x ( t ) + W in u ( t + 1 ) (21) x ( t + 1 ) = ( 1 - γ ) x ( t ) + γ f ( z ( t + 1 ) ) (22) y ( t + 1 ) = f out ( W out [ x ( t + 1 ) ; u ( t + 1 ) ] ) (23) W res = S R . W λ max ( W ) where x represents the reservoir states, as well as y and u, are outputs and inputs respectively. The non-linear activation function in the reservoir (typically tanh(·)) and output (usually identity(·)) are expressed by f(·) and f out . Also, in Eq. (21) γ indicates the loss rate used to integrate the current time step's states with the previous time step's states. Moreover, in Eq. (23), SR shows the spectral radius of Wres , which is set under unity to certify the echo state feature; and the largest eigenvalue of matrix W is shown by λmax (W). In addition to the fact that training an ESN compared with other recurrent neural networks is simple and fast (because all recurrent weights are not required to be trained), an ESN is distinguished from recurrent neural networks with three major perspectives [64]: 1. A high-dimensional projection process is adopted in ESN to take the input dynamics into consideration. This leads to a similar function obtained in kernel-based learning methods (kernels) [68,69]; 2. The reservoir has a pivotal role in the whole system which possesses a significant count of sparsely linked neurons (commonly 100–1000 folds greater than the dimensions of the inputs) while none is trained. 3. Due to the linear integrations of the reservoir's echo states, straightforward linear regression techniques are utilized to calculate the linear reading layer weights (and arbitrary, the input). Training an ESN is fast and simple compared to other recurrent neural networks, which prevents the difficulties of gradient bursting and disappearing, and does not require training all recurrent weights via back-propagation during the time training phase. In addition, freezing in local minima is not possible. The large, sparsely connected reservoir remarkably enables modeling the underlying dynamics of a time series."
7,Equations of state (EOSs),"3.5 Equations of state (EOSs) An EOS is a thermodynamic expression that relates pressure, volume, and temperature (PVT) of a given substance. Thermal properties, vapor–liquid-equilibria, and volumetric behavior of pure substances or mixtures can be described usefully by EOSs. In this paper, four well-known EOSs are used to calculate the H2 solubility in alcohols with the aim of comparing them with machine learning models. PVT relationships of these EOSs, along with their parameters, are presented in Tables 3 and 4 ."
8,Assessment of models,"In this survey, the performance of models was assessed by statistical criteria, namely coefficient of determination (R2 ), root mean square error (RMSE), standard deviation (SD), maximum absolute error (MAE), and average absolute error (AAE). The formulas for these statistical parameters are presented in Appendix A. Graphic tools, which are briefly defined below, are also used to assess the performance of models: Cross plot: A cross plot allows us to see the concentration of data points around the Y = X line, which the higher the concentration of data points around this line, the better the model's performance. Trend plot: The coverage of estimated and experimental data is evaluated by plotting experimental data versus the model's estimate. The high coverage of the data can be a sign of the high quality of the model. Error distribution plot: In this graph, the error (exp-pred) is sketched versus experimental data or independent variables to see the scatter of data points around the zero-error line for discovering the possible error trend. Histogram plot: It is a statistical tool to determine the model's performance and is an indication of the discrepancy between the experimental and estimated values."
9,Results and discussion,
10,Statistical assessment of the models,"5.1 Statistical assessment of the models Five well-known statistical factors, including R2, RMSE, AAE, MAE, and SD, were used in the current study to evaluate the models' performance. The computed values for these statistical factors are listed in Table 5 . By examining the results, XGBoost, DeepESN, ELM, and MARS models can be ranked in terms of high precision, respectively. RMSE values of 0.00219 for the total dataset, 0.00148 for the training set, and 0.00392 for the testing set, along with an overall R2 value of 0.9946, reveal that the XGBoost model has the most precise estimations of H2 solubility in alcohols. Besides, the DeepESN and ELM models also have very accurate predictions. Also, XGBoost and DeepESN models have the lowest AAE and MAE for total data points, respectively. Although the calculated SD value for the XGBoost model is slightly higher than the DeepESN and ELM models, the RMSE value of this model is lower than the others, and the reason for having a higher SD is the presence of experimental values close to 0 at the denominator of this statistical parameter, and it cannot be a reason for the weaker performance of this model than the others. As mentioned earlier, the performance of developed machine learning models was compared with four EOSs. Hence, the solubility of H2 in five different alcohols with different molecular weights, acquired from the literature [23,50,52], was estimated by these intelligent models and EOSs. Predictions of machine learning models and EOSs, along with computed RMSE for them, are shown in Table 6 . Examining the RMSE values reported in Table 6, it is clear that the error range for EOSs differs from that of intelligent models, and the accuracy of intelligent models is much greater than that of EOSs. However, the Redlich-Kwong EOS had the best performance in predicting H2 solubility in alcohols compared to other EOSs. Machine learning models represent a significant superiority over EOSs, and the XGBoost model is superior to all models and EOSs with an RMSE of 0.00089."
11,Graphical assessment of the models,"5.2 Graphical assessment of the models To better evaluate the performance of the models during testing and training stages, two well-known graphical analyses, namely cross plot and error distribution, were implemented, which are shown in Fig. 6 and Fig. 7 , respectively. In cross plots, the closer the data points to the 45° line, the higher the model precision in predicting H2 solubility in alcohols. The high cloud of data around the 45° line for all proposed models indicates the high validity of these models for estimating H2 solubility in alcohols. However, it is clear from Fig. 6 that XGBoost and DeepESN models are more accurate. In the error distribution graphs, the less scattering of error around the zero-error line for a model demonstrates the good performance of that model. Again, Fig. 7 demonstrates that the XGBoost model has the least scattering of error around the zero-error line compared to other models. Also, in none of the proposed models, no error trend is seen in the training and testing stages, and most of the computed errors are around the zero-error line, which proves the accuracy of these models. In the next stage of the graphical analysis, the frequency of errors associated with all proposed models in this work for the estimation of H2 solubility in alcohols is depicted in the histograms of Fig. 8 . As shown in Fig. 8, the histogram plots of all machine learning models are free from any skewness and benefit from symmetric (normal) distributions. Also, the calculated errors for these models are rested in a limited range from −0.03 to 0.03. The bursts of growing at zero-error value can be seen in all histogram plots that indicate the excellent match between predicted and experimental values of H2 solubility in alcohols for all proposed models. However, again XGBoost and DeepESN models show less error for more data and are superior to other models."
12,Trend analysis,"5.3 Trend analysis Next, several process analyses have been performed to assess the ability of the XGBoost model to predict the actual physical process of H2 solubility in alcohols. First, the H2 solubility in methanol at a high-pressure condition and temperature of 298.15 K [20] was estimated with the XGBoost model and four EOSs, and the results are sketched in Fig. 9 . The EOSs are usually incapable of accurate predictions for H2 solubility at high-pressure or/and high-temperature conditions, as shown in Fig. 9, the solubility values at high pressures have been overestimated, which cannot make their application attractive in industrial processes. But, the XGBoost model demonstrates the superior ability to track the process of increasing solubility with increasing pressure and offers extraordinary predictions. In the next step, the H2 solubility in 1-Pentanol, which has been experimentally investigated by Brunner [50], was estimated by the XGBoost model at different operating pressure and temperature conditions, as depicted in Fig. 10 . As shown in the figure, increasing temperature and pressure have an increasing effect on the solubility of H2 in 1-Pentanol, and this impact is correctly predicted by the XGBoost model. Eventually, the solubility of H2 against pressure at a temperature of 298.15 K was estimated by the XGBoost model for three diols, namely 1,4-Butanediol, 1,3-Propanediol, and 1,2-Ethanediol with different physical properties reported in Table 1, which have been studied experimentally in the literature [51]. The “diol” term simply indicates the existence of two alcohols. The important intermediates such as diols and their ethers are used for the production of many chemical products. These compounds are utilized as solvents, moisturizers, emollients, extractors, and preservatives on a large scale in the chemical industry, and having information about the solubility of different gases in them, such as H2, is very important. As shown in Fig. 11 , the solubility of H2 in diols with higher molecular weight is higher under similar operating conditions. In this case, the XGBoost model correctly forecasts the H2 solubility trend for all three diols and has appropriate predictions at different operating pressures. Overall, the developed XGBoost model is reliable for estimating the H2 solubility in alcohols or alcohol-containing solvents with a molecular weight range of 32.042–242.446 g/mol, over a wide range of operating pressure (0.101–110.3 MPa) and temperature (213.15–524.9 K). However, the chemical structure of solvents is not considered in the modeling and is a suggestion for future work that could provide useful information on the solubility of H2 in alcohols. It is worth noting that for other alcohols or alcohol-containing solvents not available in the current database, these models may not be able to predict H2 solubility accurately. This limitation usually exists for all data-driven models."
13,Sensitivity analysis,"5.4 Sensitivity analysis To check the relative impact of the input parameters on the XGBoost model’s output, the relevancy factor (r) [73] is used in this work. The higher the r-value between any input parameter and output, the greater the effect of that parameter on H2 solubility in alcohols. The r-values are obtained using the next equation [36,74]: (24) r ( i n p i , H S ) = ∑ j = 1 n in p i , j - i n p m , i H S j - H S m ∑ j = 1 n in p i , j - i n p m , i 2 ∑ j = 1 n H S j - H S m 2 0.5 where inpi,j and inpm,i show the jth value and mean value of the ith input parameter, respectively, where i could be temperature, pressure, molecular weight, critical temperature, and critical pressure of alcohols; HSm denotes the mean value of predicted H2 solubility in alcohols and HSj is the jth value of predicted H2 solubility. The relative impact of each input parameter on H2 solubility in alcohols is presented in Fig. 12 . The pressure, temperature, and molecular weight of alcohols have a positive impact on H2 solubility in alcohols, while critical temperature and critical pressure of alcohols negatively affect the H2 solubility. Also, pressure has a greater effect on the H2 solubility in alcohols followed by temperature and molecular weight of alcohols."
14,Leverage approach execution,"5.5 Leverage approach execution In the last part of this work, the Leverage approach [75–77] is applied to recognize the applicability scope of the XGBoost model and probable outlier data. In this method, standardized residuals (R) can be calculated as follows, which represent the deviations of the estimated values by the model from the experimental data [78]: (25) R j = e j M S E 1 - H jj 0.5 where MSE denotes the mean square error of the model; while Hjj and ej , respectively, show Leverage of the jth data and deviation of the predictions from the experiments of the jth data. Afterward, the values of Hat matrix Leverage are calculated as follows [79]: (26) H = X (X T X) - 1 X T where XT is the transpose of the matrix X, which is a two-dimensional (n × p) matrix; n and p show data points count and the model dimension, respectively. The critical Leverage (H*) is also determined as 3(p + 1)/n. Then, William's plot helps to find the applicability scope of the XGBoost model and the outliers or suspected data, as shown in Fig. 13 . R-values between −3 to 3 and Hjj ≤ H* are an indication of the applicability domain of the model. Fig. 13 indicates that the majority of data are within the scope of 0 ≤ Hjj ≤ H* and −3 ≤ R ≤ 3, which proves the high reliability of the XGBoost model. The data points that are out of the domain of the model's application are reported in Table 7 . The results of the Leverage approach show that the XGBoost model has high credit in estimating the solubility of H2 in alcohols."
15,Summary and conclusions,"In the present study, a complete set of H2 solubility data (673 experimental data points) for 26 different alcohols or alcohol-containing solvents was gathered in a broad range of operating pressures and temperatures. MARS, XGBoost, ELM, and DeepESN as four robust models of machine learning were developed for the prediction of H2 solubility in alcohols as a function of temperature, pressure, molecular weight, critical temperature, and critical pressure of alcohols. The subsequent main conclusions can be drawn from this study: 1. The XGBoost model is presented as the best model for the estimation of H2 solubility in alcohols based on graphical and statistical analyses having an overall RMSE of 0.0022 and R2 of 0.9946. Also, DeepESN, ELM, and MARS models were ranked after the XGBoost model in terms of high accuracy, respectively. 2. The comparison of machine learning models and EOSs proved the superior of intelligent models for estimation of H2 solubility in alcohols. However, Redlich-Kwong EOS had the best performance compared to other EOSs. 3. The relevancy factor demonstrated that pressure has a major impact on the H2 solubility in alcohols, followed by temperature and molecular weight of alcohols. Also, temperature, pressure, and molecular weight of alcohols have a positive impact on the H2 solubility in alcohols, while critical temperature and critical pressure of alcohols have a low inverse effect on it. 4. Eventually, the results of the Leverage approach demonstrated that the XGBoost model has high credit in estimating the solubility of H2 in alcohols."
