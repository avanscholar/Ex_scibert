,sections,Text
0,Introduction,"The use of gas sensors together with pattern classification methods plays a significant role in recognition and classification of chemical compounds and detection of their harmful effects. Designing these less costly systems is important also for the industry. Alcohols constitute a huge portion of these compounds, and are used largely in hygiene products and cosmetics. In this study, 5 different types of alcohols – 1-octanol, 1-propanol, 2-butanol, 2-propanol and 1-isobutanol – are classified using QCM (quartz crystal microbalance) sensors of different structures. Among these alcohols, 1-octanol is known as fatty alcohol. Its chemical formula is CH 3 ( CH 2 ) 7 OH , and with its strong odor it is generally used in perfumes and flavorings. 1-propanol is an aliphatic alcohol with a chemical formula of CH 3 CH 2 CH 2 OH . While it very much resembles ethyl alcohol chemically, it has a strong and toxic odor. It is rather used as a solvent. 2-butanol is called as secondary alcohol, and has a formula of CH 3 CH ( OH ) CH 2 CH 3 . Again this alcohol is a strong solvent. 2-propanol is an isopropyl alcohol with a formula of CH 3 CHOHCH 3 . It is a compound used at home and in industry, and an example of secondary alcohol. The last alcohol used in this study is 1-isobutanol, and has a formula of ( CH 3 ) 2 CHCH 2 OH . It is known to be used as a solvent. This study makes use of QCM sensors. QCM sensor is a physical device that is sensitive to resonance frequency Δ f changes [1]. The aim of the study is to measure the reaction of different QCM sensors to this 5 different alcohols, and to determine which type of sensor is more successful in classification of these alcohols. QCM sensors with different structures may react very differently [1]. The reason of using QCM sensors in this study is the successful reaction of these sensors to the compounds [2]. Despite their success levels, the costs of the QCM sensors are low [3,4]. In a study about identification of alcohols QCM sensors are shown to give successful results [5]. Again in a similar study, QCM sensors are successfully used in the identification of primary alcohols [6]. ANN (artificial neural networks) produce favorable outcomes in test datasets due to their learning ability. Neural networks are widely used in classification of QCM sensor data, and they are known to give successful results in chemical applications [7]. For instance, ANN is used effectively as a pattern classification technique together with QCM sensors in the classification of volatile organic compounds [8]. Classification of gas mixtures is carried out by applying neural network to sensor data from QCM sensors [9]. Again ANN produced successful results on gas mixtures [10]. QCM sensors are used to obtain odor data from binary mixtures of acetone, ethanol and trichloroethylene, and ANN is applied with a 94.3% success [11]. ANN showed a 93.87% success in a similar study with triple gas mixtures [12]. Hydrogen, methane and carbon mono-oxide are classified by using feed forward back propagation neural network and acceptable results are obtained [13]. It is possible to meet QCM sensors in many different fields, which shows the success of these sensors. For instance, QCM sensors are used in olive oil quality classification [14], classification of Chinese liquor flavors [15], recognition of volatile aldehydes [16], detection of alcohols [17], characterization of different plant-degradation processes [18] and in various studies in health care [1]. ANN is successfully applied in different studies to data from different types of sensors and also to electronic nose data. Bahraminejad et al. conducted a classification study on 4 different odor datasets which include alcohol compounds – also butanol and propanol similar to our study – and they reached a success level of 80–100% by using ANN [19]. ANN and SVM (support vector machine) are used in the classification of binary gas mixtures of Acetone and 2-propanol with a 94.7% success level [20]. Saraoglu applied ANN successfully in his e-nose system created for anesthetic dose level detection [21]. Again e-nose data and ANN are used in the classification of pecorino cheese [22]. Although neural networks are widely used in many fields with favorable success levels, the traditional backpropagation (BP) algorithm may get stuck in local minima. Hybrid algorithms are commonly used to overcome this weakness of the BP algorithm. In this study, ANN is trained by ABC (artificial bee colony) algorithm. ABC is strong in exploration feature [23], it is easy to apply and requires few parameters [24]. There are numerous studies where ABC is used in training of ANN with encouraging results. For instance, e-nose data from 4 different fruits is classified by ABC-based ANN, and a 76.39% success level is obtained [25]. ANN is used together with ABC in predicting bottom hole pressure and 99% success is reached [26]. Again ABC produced successful results in detecting oil spillage [27], and predicting annual production of hydroelectric [28], and for stock market prices prediction [29]. The contributions of this paper are summarized as follows. • In ABC MAPE (mean absolute percentage error) is being referenced instead of MSE as fitness behavior. • Performance prediction is performed during the training by ABC. • Training neural networks by ABC and specific parameters like weight bounds, momentum learning rate can be set by user and can be changed dynamically during the training. In the light of these studies, it can be concluded that ABC based ANN would produce successful results on QCM sensors data. The study is organized as follows. First section gives details about how gas data is collected and measurements are done. The methods and possible scenarios are presented in the Section 2. Results are given in the Section 3. Finally, the conclusions are addressed in the last section."
1,Data background,"1.1 Data background In this study, five different QCM gas sensors are used, and five different gas measurements (1-octanol, 1-propanol, 2-butanol, 2-propanol and 1-isobutanol) are conducted in each of these sensors. There are two different channels in these QCM sensors. One of these channel includes molecularly imprinted polymers (MIP), and the other includes nanoparticles (NP). Diverse QCM sensor structures are obtained using different MIP and NP ratios. One single measurement in any QCM sensor lasted for 120 min. This time period consists of the following processes. The sensor is cleaned by pure air in the first 30 min. The gas sample is passed through the sensor in five different concentrations. These concentrations are given in Table 1 . As it can be observed from Table 1, air ratio is gradually decreased in the measurements. Sensor is cleaned with pure air for seven minutes before measuring the next gas concentration. All the measurements are conducted at 25 °C room temperature. Liquid gas samples are filled into a glass tube of 50 ml. The liquid sample reaches the sensor in gas form, and the response of the sensor is transferred to the computer through two different channels as a digital data. Data from channel 1 and 2 are collected, and frequencies are measured in Hz to construct the dataset. QCM (MIP:1, NP:1) sensor frequency data from channel 1 and 2 for 1-propanol, is given in Fig. 1 as an example. As it can be observed from the Fig. 1, five samples are taken in each gas concentration. A complete data set is constructed through these samples."
2,Gas sensor characteristics,"1.2 Gas sensor characteristics A QCM is an electromechanical oscillator that contains a thin slice of quartz crystal with two channels placed on its surface. A chemical receptive material form on QCM surfaces, associated with analytic induces a frequency reduction in the oscillation parallel to consumed mass, given in the Sauerbrey’s equation [30,31] (Eq. (1)). In the Eq. (1) f is the shift in frequency, m is the variation of the consumed mass, Cf is the mass sensitivity constant, f0 is the root resonating frequency, and channels are represented by A. In this study, a QCM sensor is used which contains two channels. Channels are composed of imprinted polymers (MIP) and nanoparticles (NP). Different QCM sensors are obtained by changing the ratios of polymer and nanoparticles. Fig. 2 shows a sample QCM sensor used in this study. As can be seen from the Fig. 2, there are two yellow circles in the sensor. One of these circles forms channel 1, and the other forms channel 2. MIP and MP ratios used in the QCM sensors are given in Table 2 . QCM6 consists of only MIP, and QCM12 consists of only NP. Other sensors include both MIP and NP in certain ratios. (1) Δ f = - C f f 0 2 Δ m A"
3,Methods and scenarios,"ANN-ABC method, known to produce successful results, is used to classify all QCM sensor data. The method is compared with the traditional ANN-BP method, and the results are analyzed. Single hidden layered networks with different epoch and neuron numbers are tested in order to observe which gives the best results in this dataset, and hence to determine the most successful network design. For each QCM structure, networks with epoch numbers 500, 1000, 3000, 5000, 7000 and 10,000 are tested respectively, and for each of these epoch value, neuron number of the single hidden layer is changed from 10 up to 100 by a 10 increase each time. Therefore, for each epoch number 10 different network designs are tested. The reason of starting epoch numbers from 500 and finished in 10,000 is obtaining low performance for other values. Again for neuron number of the single hidden layer, according to results, 10–100 is the best range for ANN-BP and ANN-ABC."
4,Artificial Neural Network (ANN),"2.1 Artificial Neural Network (ANN) Artificial neural networks are proposed to simulate working principles of the brain. Elements (nodes) performing simple operations acquire data processing and calculation capabilities when they cooperate with each other [32]. ANN consists of layers, neurons that form layers, and connections called weights that connect neurons. Backpropagation (BP) algorithm is used traditionally in the training phase. ANN is composed of three basic layers. These are the input layer, hidden layer and the output layer. There can be one or more hidden layers. Artificial neural network traditionally uses BP algorithm to update the weights. First, feedforward is applied to the network and then training done by BP algorithm and the weights are updated. The error is back propagated in the network in order to update the weights and thresholds. During feedforward, NET value is calculated using Eq. (2). wkj represents the weights associated with the kth neuron, and o k i represents the output value of that neuron. (2) NET j a = ∑ k = 1 n w kj o k i After the NET values are calculated, they are transferred by an activation function to calculate the hidden layer outputs. Net output values are calculated by multiplying the hidden layer outputs with the weights which connect the hidden layer to the output layer. Similarly, these NET output values are put into the activation function to calculate the network output (Eq. (3)). β j a represents the threshold value of the associated neuron. Sigmoid function is used as the activation function in this study. (3) o j a = 1 1 + e - NET j a + β j a MSE (mean squared error) is used to calculate the error during the training of the network. One epoch is completed when all training data is given to the network. Error value is calculated in each epoch, and all of them are summed up and divided by the size of the training dataset (Eq. (4)). (4) Total Error = 1 2 ∑ m E m 2"
5,Artificial Bee Colony algorithm (ABC),"2.2 Artificial Bee Colony algorithm (ABC) Artificial bee colony algorithm is a swarm optimization algorithm, and it simulates nectar searching behaviors of the bees. It is first developed by Karaboga [33]. ABC includes three types of bees, these types are employed, onlooker, and scout bees. The algorithm has certain assumptions such as each source is controlled by only one employed bee, number of employed and unemployed bees are equal, and any employed bee turns to a scout bee if its source is depleted. Employed bees collect the nectars found out in the new sources. Scout bees search for new nectar in the light of the information they obtain from onlooker bees. Onlooker bees search for new nectar source areas, and they turn into employed bees once they find an area. An employed bee turns into a scout bee when the newly found nectar source is exploited. Food sources are generated randomly in the first step of the algorithm within given upper and lower limits. The Eq. (5) uses the rand function to generate random numbers while producing food sources, i represents the food source in the equation, while j represents the number of parameters to be optimized. (5) x ij = x min j + rand 0 , 1 ∗ x max j - x min j Employed bees turn into scout bees when the nectar source depletes, and they determine the new nectar source referring the information onlooker bees give. They use Eq. (6) during this process. v ij represents the new nectar location in Eq. (6). Index i represents the random food source, and index j represents number of parameters to be optimized. ϕ ij is a random number and it controls the generation of neighboring food sources. (6) v ij = x ij + ϕ ij x ij - x kj Suitability of the new food source is calculated by the fitness function in Eq. (7). If the nectar amount of the new food source is lower than of existing one then the current food source is not changed, and the search continues for a new source. (7) fitness i = 1 1 + f i f i ≥ 0 1 abs f i f i < 0 Suitability f i in Eq. (7) is the fitness value of the i th food source, and it is related with the nectar amount in that source. ABC algorithm runs for a predetermined number of iterations and it aims to find the global minimum."
6,ANN trained with ABC,"2.3 ANN trained with ABC Besides having a broad range of application areas, artificial neural networks have certain weaknesses like getting stuck in local minima and over-fitting the training dataset [34]. Numerous studies have been conducted to overcome these problems. Most of these studies use hybrid algorithms. In this study, instead of backpropagation (BP) algorithm, ABC algorithm is used to train the artificial neural network. It is chosen because it can easily be adapted to ANN, and relatively low number of parameters are required. ABC is a strong algorithm in terms of exploration, and therefore it will reach global minimum without getting stuck in local minima. The Fig. 3 shows how ANN-ABC creates the array of is being optimized by ABC algorithm. Elements of array consist by weights and threshold values of neural network. Beside this for summarizing the method the flow chart in Fig. 4 shows the step by step procedure of how ANN is trained by ABC. The algorithm takes randomly generated weights and thresholds within an array, it optimizes and finds the most suitable weights and thresholds. Suitability, at each iteration, is calculated by feedforward movement in the network using the weights and thresholds of that iteration. In ANN-ABC unlike ANN-BP, MAPE approach is used to calculate fitness of network. ANN-BP and ANN-ABC parameters used throughout this study are given in Table 3 . In BP, momentum and learning coefficient are taken as 0.8 and 0.2, respectively. In ABC, colony size and food source limit are taken as 50 and 250, respectively. A food source area represents an achievable solution to the problem that is to be optimized so increase the food source in a certain rate will benefit in large scale problems by increasing reach nectar area possibility. Each scenario is tested three times using the same parameters and dataset to obtain the average values. 60% of the dataset is used for training purposes, and the rest (40%) is used for testing. These training and testing datasets are chosen randomly in each scenario. Since the Sigmoid function is used as the activation function in ANN, dataset values are normalized in the range [0 1], using the min-max normalization given in Eq. (8). xi represents the number to be normalized, xmin is the minimum, and xmax is the maximum of the dataset. (8) x i , 0 - 1 = x i - x min x max - x min"
7,Results and discussion,"A total of 60 scenarios were trained and tested for each different structure of QCM sensor. Details of scenarios were described in Section 2. Best and lower MSE values for QCM scenarios were reached in the test dataset are given in Table 5. The best scenario in QCM3 was realized in ANN-ABC with an MSE value of 2.33E−16 in test dataset. This error value was reached with 5000 maximum cycle numbers (MCN) and 40 neurons in hidden layer. The closest MSE value in ANN-BP was obtained as 4.18E−06 with 10,000 epochs and 60 neurons. The lowest MSE value ANN-BP reached in training phase is 3.65E−06, while it was 5.69E−16 for ANN-ABC. When comparing results with similar studies that mentioned in introduction part. In [25,27,28] their methods are close to our approach but we use MAPE instead of MSE in ANN-ABC training and get better results in classification of gases that can be seen in Table 4 . Referring to the Table 5 , for QCM6, the best scenario was realized in ANN-ABC with an MSE value of 1.41E−16. This error value was obtained with a network trained for 5000 MCN and has 70 neurons in hidden layer. The closest ANN-BP scenario network was the one with 3.94E−06 MSE value in test dataset, trained for 10,000 epochs and has 70 neurons. The lowest MSE value ANN-BP reached in training phase was 3.81E−06, while it was 3.13E−16 for ANN-ABC. In QCM7, the best scenario was realized in again ANN-ABC with an MSE value of 1.53E−16 in test dataset. This error value was reached with a network trained for 5000 MCN and has 30 neurons in hidden layer. The closest ANN-BP scenario network was the one with 3.23E−06 MSE value in test dataset, trained for 10,000 epochs and has 40 neurons. The lowest MSE value ANN-BP reaches in training phase was 5.43E−06, while it was 8.77E−16 for ANN-ABC. In QCM10, the best scenario was realized in again ANN-ABC with an MSE value of 1.74E−16 in test dataset. This error value was reached with a network trained for 7000 MCN and has 40 neurons in hidden layer. The closest ANN-BP scenario network was the one with 4.05E−06 MSE value in test dataset, trained for 10,000 epochs and has 80 neurons. The lowest MSE value ANN-BP reached in training phase was 4.01E−06, while it was 8.18E−16 for ANN-ABC. In QCM12, the best scenario was realized in again ANN-ABC with an MSE value of 4.20E−16 in test dataset. This error value was reached with a network trained for 10,000 MCN and has 70 neurons in hidden layer. The closest ANN-BP scenario network was the one with 5.52E−06 MSE value in test dataset, trained for 7000 epochs and has 50 neurons. The lowest MSE value ANN-BP reached in training phase was 7.73E−06, while it was 4.97E−16 for ANN-ABC. MSE graphs of the training sets of all best two scenarios were given in Fig. 5 . Some part of the graph for specific epochs, in Fig. 5, is zoomed in to distinguish the lowest training error value reached. The findings given above were to show the QCM structure which is the most successful in classification. Another aim of this study is to identify, among the 5 different gas types, the one which can be classified with a higher success rate. The results from all of the 5 different QCM sensors are taken into account during these tests. All of the 5 different gasses are classified more successfully by ANN-ABC rather than ANN-BP. ANN-ABC is able to classify the 5 gasses with a success rate of over 99%. It performed best during the classification of 1-octanol and 1-isobutanol alcohols compared to ANN-BP, as can be seen in Fig. 6 . Average test-data performance is 99.29% using ANN-BP, while it is 99.73% for ANN-ABC. Although the overall level of performance seems to be very close for both of them, the algorithm that is most successful in classification of a particular gas type can be observed more clearly in Fig. 7 and in Fig. 6. BP-Act (actual). ANN-ABC gives the closest results in 5 actual test-datasets. The classification performance is represented in the dataset as follows. The row of 1-octanol, for example, would include [1 0 0 0 0], assuming that the first column is for 1-octanol, where each column represents an output. The performance of an algorithm in this study is measured by not only its proximity to or exact determination of the value of 1 in the respective column, but also by its proximity to or exact determination of the value of zero in the remaining four columns. The outputs are measured using this approach and the related graphics are given in Fig. 7. The axes are the values of real data. It can be seen that ABC gets closer to actual data than BP, and it even overlaps in most of the results. According to these measurements new performance value for ANN-BP is 94.75% and for ANN-ABC is 98.84%. When each alcohol is considered separately, results of BP get quite far away from the actual results in the case of 1-octanol and 1-butanol, particularly, whereas ABC gives fairly close results for these alcohols. The strength of exploration capability of ABC is well known and when combined with the strengths of ANN, much more accurate results were obtained."
8,Conclusion,"In this study, data of five different alcohols (1-octanol, 1-propanol, 2-butanol, 2-propanol and iso-butanol) are obtained by using QCM sensors. Five different QCM sensor structures are used, and the datasets obtained through these sensors are classified by using ANN trained with ABC. ANN-ABC gives much more successful results compared to traditional ANN-BP method, and it reaches E-16 MSE levels in training dataset. ANN-BP, however, remains in E-06 MSE levels at the minimum in training phase. ANN-ABC achieves the same level of success also in test dataset, and reaches E-16 MSE levels; while ANN-BP remains in E-06 MSE levels also in the test phase. This study shows that different alcohols obtained from QCM sensors with different structures can be classified successfully by using ANN-ABC method."
